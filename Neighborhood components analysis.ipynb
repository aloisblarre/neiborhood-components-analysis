{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neighborhood components Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook has been made to illustrate the [paper](https://cs.nyu.edu/~roweis/papers/ncanips.pdf) Jacob Goldberger, Sam Roweis, Geoff Hinton, Ruslan Salakhutdinov. I strongly recommend reading this paper before playing that notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This paper presents a novel non parametric method for learning the Mahalanobis distance. This method addresses the two major issues encountered with KNN classification :\n",
    "- the computational isssue by reducing the dimension of the problem\n",
    "- the definition of a \"near\" neighbor by providing a new quadratic distance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this paper and show its performances, we will use the NIST dataset for the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Some of the exercices in this notebook can be hard if you haven't read the relative paper. I recommend not spending too much time stuck on a question, load the solution if you spend more than 5 minutes stuck and most importantly try and understand the idea of what we are doing. \n",
    "    \n",
    "## Don't stay stuck, don't get lost !\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "y = digits.data\n",
    "d = digits.target\n",
    "X_train, X_test, c_train, c_test = train_test_split(y, d, test_size=0.3)\n",
    "\n",
    "D = X_train.shape[1]\n",
    "n = X_train.shape[0]\n",
    "\n",
    "print(f'Number of fearures (D) : {D}')\n",
    "print(f'Number of vectors (n) : {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Nearest Neighbours for Distance Metric Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a labeled data set consisting of n real-valued input vectors $x_1$, . . . , $x_n$ in $R^D$\n",
    "and corresponding class labels $c_1$, ..., $c_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We here aim at optimizing the Leave-One-Out (LOO) classification error on the training data. First, we restrict ourselves to learning  Mahalanobis (quadratic) distance metrics, which can always be represented by symmetric positive semi-definite matrices. Then, if we denote the transformation by a matrix $A$ we are effectively learning a metric $Q = A^TA$ such that $d(x, y) = (x − y)^T Q(x − y) = (Ax − Ay)^T (Ax − Ay)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Initialize a matrix $A$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> To get better performances, you can initialize A to the identity matrix.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/matrix_A.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the LOO classification error continuous, we have to make the neighbor selection continuous. To do so, each point $i$ selects another point $j$ as its neighbour with some probability $p_{ij}$, and inherits its class label from the point it selects. After applying a softmax, we have :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "p_{ij} = \\frac{\\exp(−{\\lVert Ax_i − Ax_{j}\\rVert}^2)}{\\sum_{k\\ne i}{\\exp(−{\\lVert Ax_i − Ax_{k}\\rVert}^2))}}, \\;\\; p_{ii} = 0\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Compute $p_{ij}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_ij = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/p_ij.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the probability $p_i$ that point $i$ is going to be correctly classified :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "p_{i} = \\sum_{i \\in C_i}{p_{ij}}, \\;\\;\\; C_{i} = \\{j|c_i = c_j\\}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Compute $p_{i}$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_class_mask = ...\n",
    "masked_p_ij = ...\n",
    "p = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/p_i.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we aim at correctly classifying a maximum of points, our objective function is :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "f(A) = \\sum_{i}{\\sum_{j \\in C_i}{p_{ij}}} = \\sum_{i}{p_i} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Compute the objective function\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/objective_function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating $f$ with respect to the transformation matrix $A$ yields a gradient rule which\n",
    "we can use for learning :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{∂f}{∂A} = 2A \\sum_{i}{\\left(p_i \\sum_{k}{p_{ik}x_{ik}x_{ik}^T} - \\sum_{j \\in C_i}{p_{ij}x_{ij}x_{ij}^T}\\right)}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Implement this gradient rule\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/gradient.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Let's put all that in a function to compute the gradient\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(A,x, same_class_mask):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/gradient_function.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Minimise the loss\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/optimizer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned the transformation, we can evaluate its performance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Compare the performance of a KNN on the raw data with one of a KNN on the transformed data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/performance.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Low Rank Distance Metrics and Nonsquare Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other main advantage to NCA is its performance in reducing the dimensionnality of the input data. It can be very useful to speed up computations in case of large input data dimensionnality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do so, all we need is to reduce the output size of a from the number of features to the dimentionnality we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Take all the work you have done so far and modify the output size of A to 2 to reduce the dimensionnality of the input vectors\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> To get better performances, you can initialize A to PCA transformation matrix.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rectangle_matrix.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Check the performance of a KNN on the data after a PCA transformation with one of a KNN on the data after a NCA transformation\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/rectangle_performance.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now see how much more efficient the NCA transformation is compared to a classic PCA. Here is a chart from the original paper showing the performance of NCA :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/performance.png\" alt=\"Trulli\" style=\"width:100%\">\n",
    "  <figcaption>KNN classification accuracy (left train, right test) on UCI datasets balance, ionosphere, iris, wine and housing and on the USPS handwritten digits. Results are averages\n",
    "over 40 realizations of splitting each dataset into training (70%) and testing (30%) subsets\n",
    "(for USPS 200 images for each of the 10 digit classes were used for training and 500 for\n",
    "testing). Top panels show distance metric learning (square A) and bottom panels show\n",
    "linear dimensionality reduction down to d = 2.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an illustration of the transformation made by NCA compared to PCA and LDA :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"images/transformation_illustration.png\" alt=\"Trulli\" style=\"width:100%\">\n",
    "  <figcaption>Dataset visualization results of PCA, LDA and NCA applied to (from top) the\n",
    "“concentric rings”, “wine”, “faces” and “digits” datasets. The data are reduced from their\n",
    "original dimensionalities (D=3,D=13,D=560,D=256 respectively) to the d=2 dimensions\n",
    "show.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit-learn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this paper has been published, it has been implemented in scikit-learn. [Here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit) is the documentation of the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Use the scikit-learn implementation on the digits dataset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/sklearn_implementation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play with a little illustration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try and reproduce part of the illustration above with a part of the digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits(n_class=5) #We reduce the number of selected digits to reduce the number of classes\n",
    "y = digits.data\n",
    "d = digits.target\n",
    "X_train, X_test, c_train, c_test = train_test_split(y, d, test_size=0.3)\n",
    "\n",
    "D = X_train.shape[1]\n",
    "n = X_train.shape[0]\n",
    "\n",
    "print(f'Number of fearures (D) : {D}')\n",
    "print(f'Number of vectors (n) : {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "**Exercice:** Plot the data projected un a 2D space with PCA, LDA and NCA\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "X_test_pca = ...\n",
    "X_test_lda = ...\n",
    "X_test_nca = ...\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(20, 7))\n",
    "ax[0].scatter(X_test_pca[:,0], X_test_pca[:,1], c=c_test)\n",
    "ax[0].set_xlabel('PCA')\n",
    "ax[1].scatter(X_test_lda[:,0], X_test_lda[:,1], c=c_test)\n",
    "ax[1].set_xlabel('LDA')\n",
    "ax[2].scatter(X_test_nca[:,0], X_test_nca[:,1], c=c_test)\n",
    "ax[2].set_xlabel('NCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/illustration.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can play with the number of digits you keep and see how different and better the NCA transformation is compared to the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Easier to compute objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing the objective function $f(A)$ is equivalent to minimizing the $L_1$ norm between\n",
    "the true class distribution (having probability one on the true class) and the stochastic class\n",
    "distribution induced by $p_{ij}$ via $A$. A natural alternative distance is the KL-divergence which\n",
    "induces the following objective function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "g(A) = \\sum_{i}{log\\left(\\sum_{j \\in C_i}{p_{ij}}\\right)} = \\sum_{i}{log(p_i)} \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximizing this objective would correspond to maximizing the probability of obtaining a\n",
    "perfect (error free) classification of the entire training set. The gradient of $g(A)$ is even\n",
    "simpler than that of $f(A)$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation*}\n",
    "\\frac{∂g}{∂A} = 2A \\sum_{i}{\\left(\\sum_{k}{p_{ik}x_{ik}x_{ik}^T} - \\frac{\\sum_{j \\in C_i}{p_{ij}x_{ij}x_{ij}^T}}{\\sum_{j \\in C_i}{p_{ij}}}\\right)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other dimension reduction algorithms (relation to previous other works)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisher introduced in his [paper](https://www.comp.tmu.ac.jp/morbier/R/Fisher-1936-Ann._Eugen.pdf) in 1936, the Fisher's linear discriminant which is the base for Linear discriminant analysis (LDA). It is a method used to find a linear combination of features that characterizes or separates two or more classes. The resulting combination may be used for classification or dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA dimensionality reduction matrix can be used as an initialization of the matrix A in NCA in order to find a better local optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant components analysis, introduced in 2002 by Noam Shental, Tomer Hertz, Daphna Weinshall, and Misha Pavel in this [paper](https://link.springer.com/content/pdf/10.1007/3-540-47979-1_52.pdf) and later used in this [paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.5.3038&rep=rep1&type=pdf) is a method which aims at finding a transformation that amplifies relevant variability and suppresses irrelevant variability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neither of these method are as efficient as NCA in dimension reduction for classifying data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
